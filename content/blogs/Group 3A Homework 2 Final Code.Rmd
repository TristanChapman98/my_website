---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-09-30"
description: Code to Present Alcohol Consumption Data # the title that will show up once someone gets to this page
draft: false
image: image22.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: Bikes # slug is the shorthand URL address... no spaces plz
title: TFL Bike Rentals
---



```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(grid)
library(ggpubr)
library(rvest)
library(ggtext)
```

#Question 1: Climate change and temperature anomalies 

Use data to study climate change between 1951-1980

```{r weather_data, cache=TRUE}

# Load data of weather data of each country
weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, # The real data table only starts in Row 2, so we need to skip one row. 
           na = "***") # `na = "***"` option informs R how missing observations in the spreadsheet are coded
```

```{r tidyweather}

# Convert the data frame from wide to 'long' format
tidyweather <- weather %>% 
  select("Year","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec") %>% #Select the year and the twelve month variables from weather dataset
  pivot_longer(cols = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"), 
               names_to = "month",
               values_to = "delta")  #name the variable and the temperature deviation values
```

## Plotting Information

```{r scatter_plot}

# Create variable `date` in order to ensure that the `delta` values are plot chronologically
tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")), 
         month = month(date),
         year = year(date))

# Create a time-series scatter plot of the climate change
ggplot(tidyweather, aes(x = date, y = delta))+
  geom_point()+
  geom_smooth(color="red") + #add a red trendline
  theme_bw() + #theme
  labs (
    title = "Weather Anomalies"
  )  #create title and axis labels for the graph

```

```{r facet_wrap}

# Create the labeller for each month to be shown in the following plots
month_name <- list(
  "1" = "Jan",
  "2" = "Feb",
  "3" = "Mar",
  "4" = "Apr",
  "5" = "May",
  "6" = "Jun",
  "7" = "Jul",
  "8" = "Aug",
  "9" = "Sep",
  "10" = "Oct",
  "11" = "Nov",
  "12" = "Dec"
)
month_labeller <- function(variable,value){
  return(month_name[value])
}

# Produce separate scatter plots for each month in terms of the climate change
ggplot(tidyweather, aes(x = date, y = delta))+ 
  geom_point() + 
  geom_smooth(color="red") + #add red trendlines
  facet_wrap(~ month, ncol = 4, nrow = 3, labeller = month_labeller) + #facet by month and label each plot with the abbreviated month name
  theme_bw() + #theme
  labs (
    title = "Weather Anomalies",
    x = "Year",
    y = "Temperature Deviation"
  ) #create title and axis labels for the graph
NULL

```

```{r intervals}

# Create variable "interval" as decades
comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

```{r density_plot}

# Create density plot of climate change over decades
ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with transparency set to 20%
  theme_bw() +                #theme
  labs (
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density"         #changing y-axis label to sentence case
  )

```

```{r averaging}

# Creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta = mean(delta, na.rm = TRUE))

# Plotting the data:
ggplot(average_annual_anomaly, aes(x = Year, y = annual_average_delta))+
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_smooth() +
  
  #change to theme_bw() to have white background + black frame around plot
  theme(
    panel.background = element_rect(fill = 'white', colour = 'black')
  ) +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  ) 
NULL

```

## Confidence Interval for `delta`

NASA points out on their website that a one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

```{r, calculate_CI_using_formula}

# Using a formula to construct a 95% CI for the delta
formula_ci <- comparison %>% 
  filter(interval == "2011-present") %>%  # choose the interval 2011-present 
  filter(!is.na(delta)) %>%  # eliminate NA (not available) values 
  summarize(mean_delta = mean(delta),
            sd_delta = sd(delta),
            count = n(),
            t_critical = qt(0.95, count-1),
            se_delta = sd(delta)/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean_delta - margin_of_error,
            delta_high = mean_delta + margin_of_error
            ) 
  # calculate summary statistics for temperature deviation (delta) 
  # calculate mean, SD, count, SE, lower/upper 95% CI

formula_ci #print out formula_CI 
```

```{r, calculate_CI_using_bootstrap}

# Use the infer package to construct a 95% CI for delta
bootstrap_ci <- comparison %>% 
  filter(interval == "2011-present") %>%  # choose the interval 2011-present 
  specify(response = delta) %>% # Specify the variable of interest
  generate(reps = 1000, type = "bootstrap") %>% # Generate a bunch of bootstrap samples
  calculate(stat = "mean") # Find the median of each sample

boot_ci <- bootstrap_ci %>% 
  get_confidence_interval(level = 0.95, type = "percentile") # Get the lower and upper 95% CI

boot_ci #print out boot_ci_CI
```

Conclusion: The 95% confidence interval of average annual delta since 2011 is [1.02, 1.1]. Using the formula and using the bootstrap simulation can generate the same result on that. 

#Question 2: Biden's Approval Margins

Generating a graph using poll data that tracks the president's approval margin

```{r, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('approval_polllist.csv')

glimpse(approval_polllist)

# Using `lubridate` to fix dates, as they are given as characters in the dataset.
approval_polllist <- unique(approval_polllist) %>%
  mutate(modeldate = mdy(modeldate),
         startdate = mdy(startdate),
         enddate = lubridate::week(mdy (enddate)),
         approval_margin = (approve-disapprove)) #calculate and create column for approval margin 

  #group week by average approval margin 
  new_approval_polllist <- approval_polllist %>% 
    group_by(enddate)%>% #groups by week of year 
  
  #find statistics to generate 95% confidence interval of approval margin
    summarize(margin_mean = mean(approval_margin), #finds mean approval margin
              n = count(enddate), #counts number of polls in a given week
              margin_sd = sd(approval_margin), #finds standard deviation of approval margin
              
  #calculate the tail end of 97.5% confidence interval which is added to +_ the mean to generate a 95% interval
              error = qnorm(0.975)*margin_sd/sqrt(n), 
              top = margin_mean + error, 
              bottom = margin_mean - error)
```

## Create a plot

```{r trump_margins, echo=FALSE, out.width="100%"}
#what graph should look like
knitr::include_graphics(here::here("images", "biden_approval_margin.png"), error = FALSE)
```
```{r echo = FALSE,fig.width = 6, fig.height= 5}

#creating a plot for Biden's approval rate
approval_polllist%>% 
  ggplot(data = new_approval_polllist, mapping = aes(y = margin_mean, x = enddate)) + #creates plot 
  geom_ribbon(aes(ymin=bottom, ymax= top), fill = 'gray93', color= 'red', lwd = .2)+ #creates shaded region for CI
  geom_point(color= 'red') + #adds scatter plot to graph
  geom_line(color = 'red', lwd = .2)+ #adds a line between data points
  geom_smooth(color= 'blue', se= FALSE)+ #adds a regression line for data points
  geom_hline(yintercept=0,color = 'orange')+ #adds orange line at zero 
  labs( #generates graph titles
    title = "Estimating Approval Margin (approve - disapprove) for Joe Biden",
    subtitle = "Weekly average of all polls",
    y = "Average Approval Margin (Approve - Disapprove)" ,
    x = "Week of the year", 
  )+ 
  theme_bw()+ #adds black and white theme
  annotate("text", x = 23.5, y= 21, label = "2021") #annotates graph with year
```

#Question 3: Global warming and political views (GSS)

Using a 2010 Pew Research poll, we analyze whether there are any differences between the proportion of people who believe the earth is getting warmer and their political ideology.

```{r, read_global_warming_pew_data}
global_warming_pew <- read_csv(here::here("data", "global_warming_pew.csv"))
```


```{r}
global_warming_pew %>% 
  count(party_or_ideology, response) #take a look at the original answer
```

We constructed three 95% confidence intervals to estimate population parameters, for the % who believe that Earth is warming, according to their party or ideology. We excluded the people who wrote dont know / refuse to answer!

```{r}
global_warming_pew_excl_na <- subset(global_warming_pew,response!="Don't know / refuse to answer") #to exclude the answers that should not be taken into considerations (like "No Answer", "Don't Know", "Not applicable", "Refused to Answer")
global_warming_pew_excl_na %>% 
  count(party_or_ideology, response) %>% #count the 
  pivot_wider(names_from = response,
              values_from = n) %>% #change the data into wide format
  janitor::clean_names() %>% 
  mutate(
    total = earth_is_warming + not_warming, #count the sample size for each sample (party or ideology)
    prop = earth_is_warming/total, #calculate proportion of "earth is warming" in each sample
    se = sqrt(prop*(1-prop)/total), #calculate standard error of each sample, the data has been converter into a binomial distribution so this formula is used
    lower_ci = prop-1.96*se, #calculate the lower side of the confidence interval
    upper_ci = prop+1.96*se #calculate the upper side of the confidence interval
  )
```
The data shows that whether or not a respondent believes the earth is warming is dependent of their party ideology. In fact, when ranked based on the proportion of people that believe the earth is warming (taking into account the 95% confidence interval), Liberal Democrats come first,then Moderate Democrats, then Mod Republicans, and then Conservative Republicans. 


# Challenge 1: Excess rentals in TfL bike sharing

Recall the TfL data on how many bikes were hired every single day. We can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```



We can easily create a facet grid that plots bikes hired by month and year.

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

Look at May and Jun and compare 2020 with the previous years. What's happening?

Up until the year 2020, the variability of the distribution of the number of bikes hired in May and June was relatively low. In 2020 May and June the country was put in lockdown, therefore the variability in the distribution increased heavily. There were more extreme values as some days there were people riding very few bikes, and on others they were hired in high volumes. Before the lockdown, commuters may have hired bikes at similar rates everyday. Under lockdown, commuters were required to work from home and therefore on weekdays were unlikely to hire a bike. Moreover, on weekends it would be likely that bikes were hired in large numbers as riding a bike outdoors was one of the only permitted activities during this time. 


However, the challenge I want you to work on is to reproduce the following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```


```{r,bikecompiled}

biketable <- bike %>% 
  filter(year %in% 2016:2019) %>% 
  group_by(month) %>% 
  summarise(expected_rentals = mean(bikes_hired))
biketable

actualbikes <- bike %>% 
  filter(year >= 2016) %>% 
  group_by(year,month) %>% 
  summarise(actual_bikes = mean(bikes_hired)) %>%
  left_join(biketable, by = "month") %>% 
  mutate(excessrentals = actual_bikes - expected_rentals)
actualbikes


```

```{r, graph4}
actualbikes$positive <- ifelse(actualbikes$excessrentals<0,0,actualbikes$excessrentals) #if statement to plot either 0 or the excess rentals if excess rentals are negative, only 0 is plotted

actualbikes$negative <- ifelse(actualbikes$excessrentals>0,0,actualbikes$excessrentals) #if statement to plot 0 if actual bikes hired are higher than expected

graph2 <- ggplot(data = actualbikes, mapping = aes(x = month, group = year))+  #ggplot to set the axis and group by year
            geom_line(actualbikes, mapping = aes(y=expected_rentals), color = "blue")+ #aesthetics for the plots
            geom_line(actualbikes, mapping = aes(y = actual_bikes), color = "black")+
            geom_ribbon(mapping = aes(ymin = expected_rentals,
                            ymax = expected_rentals+negative), fill = "red", alpha =0.4)+
            geom_ribbon(mapping = aes(ymin = expected_rentals, ymax = expected_rentals+positive), fill = "green", alpha =0.4)+
            facet_wrap(~year)+   #faceting by year
  theme_bw()+
            theme(legend.position = "none",     #font and line sizing and theme settings
                  strip.background = element_blank(),
                  panel.border = element_blank(),
                  plot.title = element_text(size = 7),
                  plot.subtitle = element_text(size = 6),
                  strip.text.x = element_text(size = 4),
                  axis.text.y = element_text(size = 4),
                  axis.text.x = element_text(size = 4))+
            labs(title = "Monthly changes in Tfl bike rentals",          # axis titles
                subtitle = "Change from monthly average shown in blue and calculated between 2016-2019",
                 x = "Month",
                 y = "Bike rentals")+
NULL
            


graph2

```




The second one looks at percentage changes from the expected level of weekly rentals. The two grey shaded rectangles correspond to Q2 (weeks 14-26) and Q4 (weeks 40-52).

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```
```{r, secondgraph}

weeklybikerentals <- bike %>% #filter for the correct years
  filter(year %in% 2016:2021) %>% 
  group_by(year,week) %>% #group
  summarise(weeklyaverage = mean(bikes_hired)) #take the weekly average from the bike dataset
weeklybikerentals

expectedweeklyrentals <- weeklybikerentals %>% 
  filter(year %in% 2016:2019) %>% 
  group_by(week) %>% 
  summarise(mean_weekly = mean(weeklyaverage)) #find the average across the years 2016-2019 and create new column

weeklybikerentals <- left_join(weeklybikerentals, expectedweeklyrentals, by = "week") #join the two datasets together

weeklybikerentals <- weeklybikerentals %>% #mutate new columns for the percentage change, excessbikes and for when bikes rented were below expected 
  mutate(weeklypercentchange = (weeklyaverage - mean_weekly)/mean_weekly,
         excessbikes = pmax(weeklypercentchange,0), 
          underexpected = pmin(weeklypercentchange,0))
```

```{r, ggplot of weekly}

ggplot(weeklybikerentals, aes(x = week))+ #ggplot of the weekly averages
geom_line(aes(y = weeklypercentchange))+
facet_wrap(~year)+
geom_ribbon(aes(ymin = 0, ymax = excessbikes), fill = "green", alpha =0.4)+ #geomribbon to set colours for excess
geom_ribbon(aes(ymin = underexpected, ymax = 0), fill = "red", alpha = 0.4)+ #geomribbon to set colours for underexpected
ggplot2::geom_rect(
  xmin = 13, xmax = 26,       #set the partitions for the quarterly grey backgrounds
  ymin = -Inf, ymax = Inf,
  fill = "grey", alpha = 0.02) + #tailoring the transparency for the grey
ggplot2::geom_rect(
  xmin = 39, xmax = 53,
  ymin = -Inf, ymax = Inf,
  fill = "grey",alpha = 0.02)+
  
scale_x_continuous(breaks = c(13, 26, 39, 53))+ #adjusting the scales to split them into quarterly periods
scale_y_continuous(label = scales::percent)+

labs(x = "Week", y = NULL, title = "Weekly Changes in Tfl bike rentals",          #titles, subtitles and aesthetics
     subtitle = "% change from weekly average calculated between 2016 - 2019")+ 
  theme_bw()+
  theme(text = element_text(size = 14),
        strip.background = element_rect(color = NA,
                                        fill = NA),
        panel.border = element_blank()) +
  
  NULL               
    
```



Should you use the mean or the median to calculate your expected rentals? Why?

We should use the mean number of rentals as our expected rentals, since the median is a better statistic for dataset where there are outliers which could cause skew, our dataset does not have any obvious outliers, and so the mean should suffice.

#Challenge 2: How has the CPI and its components changed over the last few years?

### Scraping the FRED website 

FRED website contains the CPI data and all of its components.
```{r, scrape_CPI_Data}

url <- "https://fredaccount.stlouisfed.org/public/datalist/843" #Assigning Fred url to a value 


# get tables that exist on the page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called Components
Components <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of CPI Components
CPI_Components <- Components[[2]] %>% # the second table on the page contains the list of all components
 select(series_id)
  
CPI_Titles <- Components[[2]] %>% 
  select(title)
```

### Using tidyquant to get economic data

```{r, getting tidyquant data}
quant_data <- tidyquant::tq_get(x = CPI_Components, get = "economic.data", from =  "2000-01-01")


```

### Calculation of yoy change of All Items CPI and select components

```{r, calculating_lag_values}
select_series <- c("CPIAUCSL", "CPIHOSSL", "CPITRNSL", "CPIFABSL", "CPIAPPSL") #Creating a vector with select CPI components and All Items CPI based on data from https://www.bls.gov/cpi/tables/relative-importance/2020.htm

yoy_change <- quant_data %>% 
  filter(series_id %in% select_series) %>% #filtering for All items and mjor CPI components
  group_by(series_id) %>% 
  mutate(year_change = price/lag(price, 12) - 1, #Calculating yoy change
         Title = case_when(   #Creating a new column to identify series_id names
    endsWith(series_id, "UCSL") ~ "All Items",
    endsWith(series_id, "HOSSL") ~ "Housing",
    endsWith(series_id, "TRNSL") ~ "Transport",
    endsWith(series_id, "APPSL") ~ "Apparel",
    endsWith(series_id, "FABSL") ~ "Food and Beverage",
    
    ),
    positive = case_when( #creating a new column to determine if value is positive or not
      year_change > 0 ~ "Yes",
      year_change < 0 ~ "No"
    )
         
         ) %>% 
  filter(date >= "2015-01-01") #filtering for dates in dataset

```

```{r, adding levels}
yoy_change$Title_L <- factor(yoy_change$Title, levels=c("All Items","Housing","Transport", "Food and Beverage", "Apparel"), labels=c("All Items","Housing","Transport", "Food and Beverage", "Apparel")) #Creating levels for CPI components based on US Bureau of Labor Statistics Data
```

```{r, graph}
CPI_plot <- ggplot(data = yoy_change, mapping = aes(x = date, y = year_change,  color = positive, group = 1))+ #Coloring based on the sign of yoy change and grouping for 1 plot
  geom_point()+
  geom_smooth(se=F)+
  facet_wrap(~Title_L, scales = "free")+ #freeing scales
  theme_bw()+
  theme(legend.position = "none")+ #removing legend
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1))+ #formatting y axis for 1 decimal
  NULL

CPI_plot <- CPI_plot+
  labs(
    title = "<b> Yearly change of US CPI (All Items) and its components</b><br>
       <span style = 'font-size:12pt'>YoY change being <span style='color:#DB5E61'>positive</span> or <span  style='color:#7abaf3'>negative</span></span>",#coloring positive and negative
    y = "YoY % Change",
    caption = "Data from St Louis Fed FRED \n https://fredaccount.stlouisfed.org/public/datalist/843"
  )+
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        plot.title = element_textbox_simple(size=16))+ #enabling html to render in markdown
  NULL

CPI_plot
```

